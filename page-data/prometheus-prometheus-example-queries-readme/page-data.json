{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/prometheus-prometheus-example-queries-readme","result":{"data":{"brainNote":{"slug":"prometheus-prometheus-example-queries-readme","title":"prometheus-prometheus-example-queries-readme","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"prometheus-prometheus-example-queries-readme\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"Purpose\"), mdx(\"p\", null, \"Prometheus is awesome, but the human mind doesn't work in PromQL. The intention of this repository is to become a simple place for people to provide examples of queries they've found useful.\\nWe encourage all to contribute so that this can become something valuable to the community.\"), mdx(\"p\", null, \"Simple or complex, all input is welcome.\"), mdx(\"h2\", null, \"Further Reading\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://prometheus.io/\"\n  }, \"Prometheus Main Site\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://prometheus.io/docs/introduction/overview/\"\n  }, \"Prometheus Docs\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/\"\n  }, \"Prometheus Alert Rules\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.robustperception.io/blog/\"\n  }, \"Robust Perception\"))), mdx(\"h1\", null, \"PromQL Examples\"), mdx(\"p\", null, \"These examples are formatted as \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/\"\n  }, \"recording rules\"), \", but can be used as normal expressions.\"), mdx(\"p\", null, \"Please ensure all examples are submitted in the same format, we'd like to keep this nice and easy to read and maintain.\\nThe examples may contain some metric names and labels that aren't present on your system, if you're looking to re-use these then make sure validate the labels and metric names match your system.\"), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Show Overall CPU usage for a server\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- record: instance:node_cpu_utilization_percent:rate5m\\n  expr: 100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m])))\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \" Often useful to newcomers to Prometheus looking to replicate common host CPU checks. This query ultimately provides an overall metric for CPU usage, per instance. It does this by a calculation based on the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"idle\"), \" metric of the CPU, working out the overall percentage of the other states for a CPU in a 5 minute window and presenting that data per \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"instance\"), \".\"), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Track http error rates as a proportion of total traffic\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- record: job_instance_method_path:demo_api_request_errors_50x_requests:rate5m\\n  expr: >\\n    rate(demo_api_request_duration_seconds_count{status=\\\"500\\\",job=\\\"demo\\\"}[5m]) * 50\\n      > on(job, instance, method, path)\\n    rate(demo_api_request_duration_seconds_count{status=\\\"200\\\",job=\\\"demo\\\"}[5m])\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \" This query selects the 500-status rate for any job, instance, method, and path combinations for which the 200-status rate is not at least 50 times higher than the 500-status rate. The rate function has been used here as it's designed to be used with the counters in this query.\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"link:\"), \" \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.digitalocean.com/community/tutorials/how-to-query-prometheus-on-ubuntu-14-04-part-2\"\n  }, \"Julius Volz - Tutorial\")), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"90th Percentile latency\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- record: instance:demo_api_90th_over_50ms_and_requests_over_1:rate5m\\n  expr: >\\n    histogram_quantile(0.9, rate(demo_api_request_duration_seconds_bucket{job=\\\"demo\\\"}[5m])) > 0.05\\n      and\\n    rate(demo_api_request_duration_seconds_count{job=\\\"demo\\\"}[5m]) > 1\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \"  Select any HTTP endpoints that have a 90th percentile latency higher than 50ms (0.05s) but only for the dimensional combinations that receive more than one request per second. We use the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"histogram_quantile()\"), \" function for the percentile calculation here. It calculates the 90th percentile latency for each sub-dimension. To filter the resulting bad latencies and retain only those that receive more than one request per second. \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"histogram_quantile\"), \" is only suitable for usage with a Histogram metric.\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"link:\"), \" \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.digitalocean.com/community/tutorials/how-to-query-prometheus-on-ubuntu-14-04-part-2\"\n  }, \"Julius Volz - Tutorial\")), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"HTTP request rate, per second.. an hour ago\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- record: instance:api_http_requests_total:offset_1h_rate5m\\n  expr: rate(api_http_requests_total{status=500}[5m] offset 1h)\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \"  The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"rate()\"), \" function calculates the per-second average rate of time series in a range vector. Combining all the above tools, we can get the rates of HTTP requests of a specific timeframe. The query calculates the per-second rates of all HTTP requests that occurred in the last 5 minutes, an hour ago. Suitable for usage on a \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"counter\"), \" metric.\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Link:\"), \" \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://ordina-jworks.github.io/monitoring/2016/09/23/Monitoring-with-Prometheus.html\"\n  }, \"Tom Verelst - Ordina\")), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Kubernetes Container Memory Usage\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- record: kubernetes_pod_name:container_memory_usage_bytes:sum\\n  expr: sum by(kubernetes_pod_name) (container_memory_usage_bytes{kubernetes_namespace=\\\"kube-system\\\"})\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \" How much memory are the tools in the kube-system namespace using? Break it down by Pod and NameSpace!\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Link:\"), \" \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://coreos.com/blog/monitoring-kubernetes-with-prometheus.html\"\n  }, \"Joe Bowers - CoreOS\")), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Most expensive time series\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- record: metric_name:metrics:top_ten_count\\n  expr: topk(10, count by (__name__)({__name__=~\\\".+\\\"}))\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \" Which are your most expensive time series to store? When tuning Prometheus, these quries can help you monitor your most expensive metrics. Be cautious, this query is expensive to run.\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Link:\"), \" \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.robustperception.io/which-are-my-biggest-metrics/\"\n  }, \"Brian Brazil - Robust Perception\")), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Most expensive time series\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- record: job:metrics:top_ten_count\\n  expr: topk(10, count by (job)({__name__=~\\\".+\\\"}))\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \" Which of your jobs have the most timeseries? Be cautious, this query is expensive to run.\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Link:\"), \" \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.robustperception.io/which-are-my-biggest-metrics/\"\n  }, \"Brian Brazil - Robust Perception\")), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Which Alerts have been firing?\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- record: alerts_fired:24h\\n  expr:   sort_desc(sum(sum_over_time(ALERTS{alertstate=`firing`}[24h])) by (alertname))\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \" Which of your Alerts have been firing the most? Useful to track alert trends.\"), mdx(\"hr\", null), mdx(\"h1\", null, \"Alert Rules Examples\"), mdx(\"p\", null, \"These are examples of rules you can use with Prometheus to trigger the firing of an event, usually to the Prometheus alertmanager application. You can refer to the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/\"\n  }, \"official documentation\"), \" for more information.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- alert: <alert name>\\n  expr: <expression>\\n  for: <duration>\\n  labels:\\n    label_name: <label value>\\n  annotations:\\n    annotation_name: <annotation value>\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Disk Will Fill in 4 Hours\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- alert: PreditciveHostDiskSpace\\n  expr: predict_linear(node_filesystem_free{mountpoint=\\\"/\\\"}[4h], 4 * 3600) < 0\\n  for: 30m\\n  labels:\\n    severity: warning\\n  annotations:\\n    description: 'Based on recent sampling, the disk is likely to will fill on volume\\n      {{ $labels.mountpoint }} within the next 4 hours for instace: {{ $labels.instance_id\\n      }} tagged as: {{ $labels.instance_name_tag }}'\\n    summary: Predictive Disk Space Utilisation Alert\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \" Asks Prometheus to predict if the hosts disks will fill within four hours, based upon the last hour of sampled data. In this example, we are returning AWS EC2 specific labels to make the alert more readable.\"), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Alert on High Memory Load\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- expr: (sum(node_memory_MemTotal) - sum(node_memory_MemFree + node_memory_Buffers + node_memory_Cached) ) / sum(node_memory_MemTotal) * 100 > 85\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \" Trigger an alert if the memory of a host is almost full. This is done by deducting the total memory by the free, buffered and cached memory and dividing it by total again to obtain a percentage. The \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"> 85\"), \" will only return when the resulting value is above 85.\"), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Link:\"), \" \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://stefanprodan.com/2016/a-monitoring-solution-for-docker-hosts-containers-and-containerized-services/\"\n  }, \"Stefan Prodan - Blog\")), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Alert on High CPU utilisation\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- alert: HostCPUUtilisation\\n  expr: 100 - (avg by(instance) (irate(node_cpu{mode=\\\"idle\\\"}[5m])) * 100) > 70\\n  for: 20m\\n  labels:\\n    severity: warning\\n  annotations:\\n    description: 'High CPU utilisation detected for instance {{ $labels.instance_id\\n      }} tagged as: {{ $labels.instance_name_tag }}, the utilisation is currently:\\n      {{ $value }}%'\\n    summary: CPU Utilisation Alert\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \" Trigger an alert if a host's CPU becomes over 70% utilised for 20 minutes or more.\"), mdx(\"hr\", null), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Alert if Prometheus is throttling\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"- alert: PrometheusIngestionThrottling\\n  expr: prometheus_local_storage_persistence_urgency_score > 0.95\\n  for: 1m\\n  labels:\\n    severity: warning\\n  annotations:\\n    description: Prometheus cannot persist chunks to disk fast enough. It's urgency\\n      value is {{$value}}.\\n    summary: Prometheus is (or borderline) throttling ingestion of metrics\\n\")), mdx(\"p\", null, mdx(\"em\", {\n    parentName: \"p\"\n  }, \"Summary:\"), \" Trigger an alert if Prometheus begins to throttle its ingestion. If you see this, some TLC is required.\"), mdx(\"hr\", null));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[{"title":"About these notes","slug":"about","childMdx":{"excerpt":"Hi, I'm  Kien Nguyen-Tuan  👋. prometheus-promql-gotchas prometheus-prometheus-promql-join prometheus-prometheus-labels-relabel prometheus…"}}],"outboundReferenceNotes":[]},"site":{"siteMetadata":{"title":"@kiennt's notes"}}},"pageContext":{"slug":"prometheus-prometheus-example-queries-readme"}},"staticQueryHashes":[]}