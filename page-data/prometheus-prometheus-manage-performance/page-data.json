{"componentChunkName":"component---node-modules-gatsby-theme-andy-src-templates-note-js","path":"/prometheus-prometheus-manage-performance","result":{"data":{"brainNote":{"slug":"prometheus-prometheus-manage-performance","title":"prometheus-prometheus-manage-performance","childMdx":{"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"prometheus-prometheus-manage-performance\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", null, \"Manage Performance\"), mdx(\"h2\", null, \"Detecting a problem\"), mdx(\"p\", null, \"The useful metrics:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"prometheus_rule_group_iterations_missed_total\"), \" can indicate that some rule groups are taking too long to evaluate.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Comparing \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"prometheus_rule_group_lats_duration_second\"), \" against \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"prometheus_rule_group_interval_seconds\"), \" can tell you which group is at fault and if it is a recent change in behaviour.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"prometheus_notifications_dropped_total\"), \" indicates issues talking to the Alertmanger.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"If \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"prometheus_notifications_queue_length\"), \" is approaching \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"prometheus_notifications_queue_capacity\"), \", you may start losing alerts.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Each service discovery mechanism tends to have a metric such as \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"prometheus_sd_file_read_errors_total\"), \"...\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"prometheus_rule_evaluation_failures_total\"), \", \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"prometheus_tsdb_compaction_failed_total\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"prometheus_tsdb_wal_corruptions_total\"), \" indicate that something has gone wrong in the storage layer.\")), mdx(\"h2\", null, \"Finding expensive metrics and targets\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Find metrics with high cardinality:\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"topk(10, count by(__name__)({__name__=~\\\".+\\\"}))\\n\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"In addition to \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"up\"), \", Prometheus adds 3 other samples for every target scrape. \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"scrape_samples_scraped\"), \" is the number of samples that were on the /metrics.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"sample_samples_post_metric_relableling\"), \" is similar, but it excludes samples that were dropped by \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"metric_relabel_configs\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The final special sample added is \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"scrape_duration_seconds\"), \", which is how long that scrape took.\")), mdx(\"h2\", null, \"Reducing load\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Drop the metric at ingestion time using \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"metric_label_configs\"), \". This still transfers the metric over the network and parses it, but it's still cheaper than ingesting it into the storage layer.\")), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"scrape_configs:\\n  - job_name: some_application\\n    static_configs:\\n      - targets:\\n          - localhost:1234\\n    metric_relabel_configs:\\n      - source_labels: [__name__]\\n        regex: expensive_metric_name\\n        action: drop\\n\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Increase \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"scrape_interval\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"evaluation_interval\"), \" (<= 2 minutes)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"If the number of samples after \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"metric_relabel_configs\"), \" is higher than \", mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"sample_limit\"), \", then the scrape will fail and the samples will not be ingested. This is disabled by default but can act as an emergency relief vavle!\")), mdx(\"h2\", null, \"Horizontal sharding\"), mdx(\"p\", null, \"The approach to horizontal sharding is to have a master Prometheus and several scraping Prometheus servers.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"global:\\n  external_labels:\\n    env: prod\\n    scraper: 2\\nscrape_configs:\\n  - job_name: my_job\\n    # Service discovery etc. goes here.\\n    relabel_configs:\\n      - source_labels: [__address__]\\n        modulus: 4\\n        target_label: __tmp_hash\\n        action: hashmod\\n      - source_labels: [__tmp_hash]\\n        regex: 2 # This is the 3rd scraper.\\n        action: keep\\n\")), mdx(\"p\", null, \"Here you can see there are 4 scrapers from the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"modulus\"), \" setting. Each scraper should have a unique external label + the external labels of the master Prometheus. The master Prometheus can then use the remote read endpoint of Prometheus itself to transparently pull in data from the scrapers:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-yaml\"\n  }, \"global:\\n  external_labels:\\n    env: prod\\nremote_read:\\n  - url: http://scraper0:9090/api/v1/read\\n    read_recent: true\\n  - url: http://scraper1:9090/api/v1/read\\n    read_recent: true\\n  - url: http://scraper2:9090/api/v1/read\\n    read_recent: true\\n  - url: http://scraper3:9090/api/v1/read\\n    read_recent: true\\n\")));\n}\n;\nMDXContent.isMDXComponent = true;"},"inboundReferenceNotes":[{"title":"About these notes","slug":"about","childMdx":{"excerpt":"Hi, I'm  Kien Nguyen-Tuan  ðŸ‘‹. prometheus-promql-gotchas prometheus-prometheus-promql-join prometheus-prometheus-labels-relabel prometheusâ€¦"}}],"outboundReferenceNotes":[]},"site":{"siteMetadata":{"title":"@kiennt's notes"}}},"pageContext":{"slug":"prometheus-prometheus-manage-performance"}},"staticQueryHashes":[]}